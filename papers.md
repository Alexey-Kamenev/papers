# Papers

## 2020-10

[The Efficiency Misnomer](https://arxiv.org/abs/2110.12894)
<details>
<summary>Short abstract</summary>
Model efficiency is a critical aspect of developing and deploying machine learning models.
</details>
<details>
<summary>Notes</summary>
</details>

[Stochastic Training is Not Necessary for Generalization](https://arxiv.org/abs/2109.14119)
<details>
<summary>Short abstract</summary>
It is widely believed that the implicit regularization of stochastic gradient descent (SGD) is fundamental to the impressive generalization behavior we observe in neural networks. In this work, we demonstrate that non-stochastic full-batch training can achieve strong performance on CIFAR-10 that is on-par with SGD, using modern architectures in settings with and without data augmentation.
</details>

[A Farewell to the Bias-Variance Tradeoff? An Overview of the Theory of Overparameterized Machine Learning](https://arxiv.org/abs/2109.02355)
<details>
<summary>Short abstract</summary>
Overparameterized models are excessively complex with respect to the size of the training dataset, which results in them perfectly fitting (i.e., interpolating) the training data, which is usually noisy. Such interpolation of noisy data is traditionally associated with detrimental overfitting, and yet a wide range of interpolating models -- from simple linear models to deep neural networks -- have recently been observed to generalize extremely well on fresh test data. Indeed, the recently discovered double descent phenomenon has revealed that highly overparameterized models often improve over the best underparameterized model in test performance
</details>
